---
title: "R Notebook for Climate Change Trends"
output: github_document
always_allow_html: true
---

Loading the libraries requied to analyze the datasets

```{r}
#loading the required packages
library(tidyverse)
library(corrplot)
library(VIM)
library(Hmisc)
library(psych)
library(ggplot2)
library(dplyr)
library(plotly)
library(GGally)
library(mice)
library(readr)
library(patchwork)
library(ggcorrplot)
library(gridExtra)
library(RColorBrewer)
```

Loading the datasets

```{r}
#loading datasets
df1 <- read_csv("global-temp-annual.csv")
df2 <- read_csv("owid-co2-data.csv")

```

Summary of dataset

```{r}
#summary of datasets
cat("Dataset 1 Dimensions:", dim(df1), "\n")
cat("Dataset 2 Dimensions:", dim(df2), "\n")
cat(strrep("-", 50), "\n")
cat("Summary of Dataset 1: \n")
summary(df1)
cat(strrep("-", 50), "\n")
cat("Summary of Dataset 2: \n")
summary(df2)
cat(strrep("-", 50), "\n")
```

Examining Datasets

```{r}

cat("Dataset 1 - First 6 rows:\n")
head(df1)
cat(strrep("-", 50), "\n")
cat("Dataset 2 - First 6 rows:\n")
head(df2)
cat(strrep("-", 50), "\n")

cat("Dataset 1 data types:\n")
sapply(df1, class)
cat(strrep("-", 50), "\n")
cat("Dataset 2 data types:\n")
sapply(df2, class)
cat(strrep("-", 50), "\n")

```

Checking for Missing Values

```{r}
#analysing missing values
missing_df1 <- df1 %>%
  summarise_all(~sum(is.na(.))) %>%
  gather(key = "Variable", value = "Missing_Count") %>%
  mutate(
    Missing_Percentage = (Missing_Count / nrow(df1)) * 100,
    Present_Count = nrow(df1) - Missing_Count,
    Present_Percentage = 100 - Missing_Percentage
  ) %>%
  arrange(desc(Missing_Count))

print("Dataset 1 - Missing Values:")
print(missing_df1)

missing_df2 <- df2 %>%
  summarise_all(~sum(is.na(.))) %>%
  gather(key = "Variable", value = "Missing_Count") %>%
  mutate(
    Missing_Percentage = (Missing_Count / nrow(df2)) * 100,
    Present_Count = nrow(df2) - Missing_Count,
    Present_Percentage = 100 - Missing_Percentage
  ) %>%
  arrange(desc(Missing_Count))

print("Dataset 2 - Missing Values:")
print(missing_df2)

#visualising missing data patterns
p1 <- ggplot(missing_df1, aes(x = reorder(Variable, Missing_Percentage))) +
  geom_col(aes(y = Present_Percentage), fill = "#17becf", alpha = 0.8) +
  geom_col(aes(y = Missing_Percentage), fill = "#ff7f0e", alpha = 0.9) +
  coord_flip() +
  labs(
    title = "Missing Data Pattern - Dataset 1",
    subtitle = "Orange = Missing, Blue = Present",
    x = "Variables",
    y = "Percentage",
    caption = paste("Total observations:", nrow(df1))
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 11, color = "gray60"),
    axis.text = element_text(size = 10),
    panel.grid.minor = element_blank(),
    plot.background = element_rect(fill = "white", color = NA)
  ) +
  scale_y_continuous(labels = function(x) paste0(x, "%"))

print(p1)


p2<- ggplot(missing_df2, aes(x = reorder(Variable, Missing_Percentage))) +
  geom_col(aes(y = Present_Percentage, text = paste("Variable:", Variable, "<br>Present:", round(Present_Percentage, 1), "%")), 
           fill = "#17becf", alpha = 0.8) +
  geom_col(aes(y = Missing_Percentage, text = paste("Variable:", Variable, "<br>Missing:", round(Missing_Percentage, 1), "%")), 
           fill = "#ff7f0e", alpha = 0.9) +
  coord_flip() +
  labs(title = "Missing Data Pattern - Dataset 2",
       x = "Variables", y = "Percentage") +
  theme_minimal() +
  theme(axis.text.y = element_blank())
ggplotly(p2, tooltip = "text")

```

Identifying duplicates and unique values in Data

```{r}
cat("Dataset 1 duplicates:", sum(duplicated(df1)), "\n")
cat("Dataset 2 duplicates:", sum(duplicated(df2)), "\n")
cat(strrep("-", 50), "\n")

check_categorical <- function(df, dataset_name) {
  char_cols <- sapply(df, function(x) is.character(x) | is.factor(x))
  char_col_names <- names(df)[char_cols]
  
  for (col in char_col_names) {
    unique_count <- length(unique(df[[col]]))
    cat(sprintf("%s - %s: %d unique values\n", dataset_name, col, unique_count))
    
    if (unique_count <= 20) {
      print(table(df[[col]], useNA = "ifany"))
      cat("\n")
    }
  }
}

check_categorical(df1, "Dataset 1")
check_categorical(df2, "Dataset 2")
cat(strrep("-", 50), "\n")

```

Handling Missing Values

```{r}
handle_missing_values <- function(df, strategy = "auto") {
  df_clean <- df

  missing_pct <- df %>%
    summarise_all(~sum(is.na(.)) / length(.) * 100)
  
  #remove columns with >50% missing values
  high_missing_cols <- names(missing_pct)[missing_pct > 50]
  if (length(high_missing_cols) > 0) {
    cat("Removing columns with >50% missing values:", paste(high_missing_cols, collapse = ", "), "\n")
    df_clean <- df_clean %>% select(-all_of(high_missing_cols))
  }
  cat(strrep("-", 60), "\n")
  retained_cols <- names(df_clean)
  cat("Retained", length(retained_cols), "columns:", paste(retained_cols, collapse = ", "), "\n")
  
  #handling remaining missing values
  for (col in names(df_clean)) {
    if (sum(is.na(df_clean[[col]])) > 0) {
      if (is.numeric(df_clean[[col]])) {
        #using median for numeric columns
        df_clean[[col]][is.na(df_clean[[col]])] <- median(df_clean[[col]], na.rm = TRUE)
      } else {
        #using mode for categorical columns
        mode_val <- names(sort(table(df_clean[[col]]), decreasing = TRUE))[1]
        df_clean[[col]][is.na(df_clean[[col]])] <- mode_val
      }
    }
  }
  
  return(df_clean)
}

df1_clean <- handle_missing_values(df1)
cat(strrep("-", 60), "\n")
cat("Dataset 1: Columns", ncol(df1), "->", ncol(df1_clean), "| Rows", nrow(df1), "->", nrow(df1_clean), "after cleaning\n")
cat(strrep("-", 60), "\n")

df2_clean <- handle_missing_values(df2)
cat(strrep("-", 60), "\n")
cat("Dataset 2: Columns", ncol(df2), "->", ncol(df2_clean), "| Rows", nrow(df2), "->", nrow(df2_clean), "after cleaning\n")
cat(strrep("-", 60), "\n")

```

Univariate Analysis

For numeric columns, we will use shapiro.test() to test if the data follows normal distribution.
Upon calculating the p-value:

- If p > 0.05: Data is likely normally distributed
- If p â‰¤ 0.05: Data significantly deviates from normal distribution

```{r}
create_univariate_plots <- function(df, dataset_name) {
  numeric_cols <- names(df)[sapply(df, is.numeric)]
  categorical_cols <- names(df)[sapply(df, is.factor)]

  for (col in numeric_cols) {
    p1 <- ggplot(df, aes(x = .data[[col]])) +
      geom_histogram(bins = 30, fill = "lightblue", alpha = 0.7) +
      ggtitle(paste("Distribution of", col)) +
      theme_minimal()
    
    p2 <- ggplot(df, aes(y = .data[[col]])) +
      geom_boxplot(fill = "lightgreen", alpha = 0.7) +
      ggtitle(paste("Boxplot of", col)) +
      theme_minimal()

    print(p1)
    print(p2)

    shapiro_test <- shapiro.test(sample(df[[col]], min(5000, length(df[[col]]))))
    cat(sprintf("%s - Shapiro-Wilk normality test p-value: %.6f\n", col, shapiro_test$p.value))
  }

  for (col in categorical_cols) {
    p <- ggplot(df, aes(x = .data[[col]])) +
      geom_bar(fill = "coral", alpha = 0.7) +
      ggtitle(paste("Distribution of", col)) +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
    print(p)
  }
}

create_univariate_plots(df1_clean, "Dataset 1")
create_univariate_plots(df2_clean, "Dataset 2")
```

Correlation analysis between numeric variables in a dataset

```{r}
analyze_correlations <- function(df, dataset_name) {
  numeric_df <- df[, sapply(df, is.numeric), drop = FALSE]
  
  if (ncol(numeric_df) < 2) {
    cat("Not enough numeric variables for correlation analysis in", dataset_name, "\n")
    return()
  }
  
  cor_matrix <- cor(numeric_df, use = "complete.obs")
  cat("=== CORRELATION MATRIX FOR", dataset_name, "===\n")
  print(round(cor_matrix, 3))
  corrplot(cor_matrix, method = "color", type = "upper", 
           order = "hclust", tl.cex = 0.8, tl.col = "black",
           title = paste("Correlation Matrix -", dataset_name))

  ggcorrplot::ggcorrplot(cor_matrix, hc.order = TRUE, type = "lower",
                        lab = TRUE, lab_size = 3, method = "circle",
                        colors = c("red", "white", "blue"),
                        title = paste("Correlation Heatmap -", dataset_name))

  strong_correlations <- which(abs(cor_matrix) > 0.7 & cor_matrix != 1, arr.ind = TRUE)
  
  if (nrow(strong_correlations) > 0) {
    cat("\nStrong correlations (|r| > 0.7) in", dataset_name, ":\n")
    for (i in 1:nrow(strong_correlations)) {
      row_idx <- strong_correlations[i, 1]
      col_idx <- strong_correlations[i, 2]
      var1 <- rownames(cor_matrix)[row_idx]
      var2 <- colnames(cor_matrix)[col_idx]
      correlation <- cor_matrix[row_idx, col_idx]
      cat(sprintf("%s - %s: %.3f\n", var1, var2, correlation))

      p <- ggplot(df, aes(x = !!sym(var1), y = !!sym(var2))) +
        geom_point(alpha = 0.6, color = "blue") +
        geom_smooth(method = "lm", color = "red", se = TRUE) +
        ggtitle(paste(var1, "vs", var2, sprintf("(r = %.3f)", correlation))) +
        theme_minimal()
      print(p)
    }
  }
  
  return(cor_matrix)
}

cor_matrix1 <- analyze_correlations(df1_clean, "Dataset 1")
cor_matrix2 <- analyze_correlations(df2_clean, "Dataset 2")
```

Cross Dataset Analysis - Correlation Matrix for investigating how global temperatures are affected by CO2 levels

```{r}
#preparing temperature data
temp_data <- df1_clean %>%
  select(Year, `Land and Ocean`) %>%
  rename(year = Year, global_temp = `Land and Ocean`) %>%
  filter(!is.na(global_temp))

#preparing CO2 data - aggregate global totals by year
co2_data <- df2_clean %>%
  group_by(year) %>%
  summarise(
    global_co2 = sum(co2, na.rm = TRUE),
    global_co2_per_capita = mean(co2_per_capita, na.rm = TRUE),
    global_cumulative_co2 = sum(cumulative_co2, na.rm = TRUE),
    global_ghg = sum(total_ghg, na.rm = TRUE),
    global_methane = sum(methane, na.rm = TRUE),
    global_nitrous_oxide = sum(nitrous_oxide, na.rm = TRUE),
    avg_temp_change_from_co2 = mean(temperature_change_from_co2, na.rm = TRUE),
    avg_temp_change_from_ghg = mean(temperature_change_from_ghg, na.rm = TRUE),
    .groups = 'drop'
  ) %>%
  filter(!is.na(global_co2) & global_co2 > 0)

combined_data <- inner_join(temp_data, co2_data, by = "year")

print(paste("Combined dataset has", nrow(combined_data), "years of data"))
print(paste("Year range:", min(combined_data$year), "-", max(combined_data$year)))
```

```{r}
#calculating correlations
correlation_vars <- combined_data %>%
  select(global_temp, global_co2, global_co2_per_capita, global_cumulative_co2, 
         global_ghg, global_methane, global_nitrous_oxide)

cor_matrix <- cor(correlation_vars, use = "complete.obs")
col <- colorRampPalette(brewer.pal(9, "Blues"))(200)
corrplot(cor_matrix, method = "color", col = col,
  addCoef.col = "white", tl.col = "black", tl.srt = 45, 
  title = "Temperature vs CO2 Correlations", 
  mar = c(0,0,3,0), number.cex = 0.8, cl.cex = 0.8, tl.cex = 0.9, 
  outline = TRUE, addgrid.col = NA)
```

Cross Dataset Analysis - Time Series Analysis for investigating how global temperatures are affected by CO2 levels